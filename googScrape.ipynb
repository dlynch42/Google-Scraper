{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Link: https://serpapi.com/blog/scrape-google-images-with-python/\n",
    "\n",
    "- os:             to return environment variable (SerpApi API key) value.\n",
    "- requests:       to make a request to the website.\n",
    "- lxml:   \t    to process XML/HTML documents fast.\n",
    "- json:   \t    to convert extracted data to a JSON object.\n",
    "- re:     \t    to extract parts of the data via regular expression.\n",
    "- urllib.request:\tto save images locally with `urllib.request.urlretrieve`\n",
    "- BeautifulSoup:\tis a XML/HTML scraping library. It's used in combo with `lxml` as it faster than `html.parser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, lxml, re, json, urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from serpapi import GoogleSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create URL parameter and request headers\n",
    "- `params`:\ta prettier way of passing URL parameters to a request.\n",
    "- `user-agent`:\tto act as a \"real\" user request from the browser by passing it to request headers. Default `requests` user-agent is a `python-reqeusts` so websites might understand that it's a bot or a script and block the request to the website. Check what's your `user-agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36\"\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"q\": \"mincraft wallpaper 4k\", # search query\n",
    "    \"tbm\": \"isch\",                # image results\n",
    "    \"hl\": \"en\",                   # language of the search\n",
    "    \"gl\": \"us\",                   # country where search comes from\n",
    "    \"ijn\": \"0\"                    # page number\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request\n",
    "Make a request, pass created request parameters and headers. Pass returned HTML to `BeautifulSoup`\n",
    "\n",
    "- `timeout=30`:\tto stop waiting for response after 30 seconds.\n",
    "- `BeautifulSoup(html.text, \"lxml\")`:\t`html.text` will return a textual HTML data and `\"lxml\"` will be set as a XML/HTML processor, not the default `html.parser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(\"https://www.google.com/search\", params=params, headers=headers, timeout=30)\n",
    "soup = BeautifulSoup(html.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data \n",
    "Only with request headers, no regular expressions\n",
    "\n",
    "The reason why it's handy is beacuse when you try directly parse data from `img` tag and `src` attribute, you'll get a base64 encoded URL which will be a 1x1 image placeholder. Not a particularly useful image resolution\n",
    "\n",
    "- `params[\"content-type\"]`:\twill create a new dict key \"content-type\" and assinged a \"image/png\" value which will return images.\n",
    "- `[img[\"src\"] for img in soup.select(\"img\")]`:\twill iterate over all img tags and extracts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_with_request_headers():\n",
    "    params[\"content-type\"] = \"image/png\" # parameter that indicate the original media type \n",
    "\n",
    "    return [img[\"src\"] for img in soup.select(\"img\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggested search results\n",
    "- Thing above actual images\n",
    "- `suggested_searches`:\ta temporary `list` where extracted data will be appended at the end of the function.\n",
    "- `all_script_tags`:\ta variable which will hold all extracted `<script>` HTML tags from `soup.select(\"script\")` where `select()` will return a list of matched `<script>` tags.\n",
    "- matched_images:\twill hold all extracted matched images data from `re.findall()` which returns an iterator. This variable is needed to extract suggested search thumbnails, image thumbnails and full-resolution images.\n",
    "- `suggested_search_thumbnails` and `suggested_search_thumbnail_encoded`:\tparses part of inline JSON where `suggested_search_thumbnail_encoded` parses actual thumbnails from partly parsed inline JSON data.\n",
    "- `zip()`:\tto iterate over multiple iterables in parralel. Keep in mind that zip is used on purpose. `zip()` ends with the shortest iterator while `zip_longest()` iterates up to the length of the longest iterator.\n",
    "- `suggested_searches.append({})`:\tto `append` extracted images data to a `list` as a dictionary.\n",
    "- `select_one()`:\tto return one (instead of all) matched element in a loop.\n",
    "- `[\"href\"]`:\tis a shortcut of accessing and extracting HTML attributes with `BeautifulSoup`. Alternative is `get(<attribute>)`.\n",
    "- `\"\".join()`:\tto join all items from in iterable into a string.\n",
    "- `bytes(<variable>, \"ascii\").decode(\"unicode-escape\")`:\tto decode parsed image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggested_search_data():\n",
    "    \"\"\"\n",
    "    https://kodlogs.com/34776/json-decoder-jsondecodeerror-expecting-property-name-enclosed-in-double-quotes\n",
    "    if you try to json.loads() without json.dumps it will throw an error:\n",
    "    \"Expecting property name enclosed in double quotes\"\n",
    "    \"\"\"\n",
    "\n",
    "    suggested_searches = []\n",
    "\n",
    "    all_script_tags = soup.select(\"script\")\n",
    "\n",
    "    # https://regex101.com/r/48UZhY/6\n",
    "    matched_images = \"\".join(re.findall(r\"AF_initDataCallback\\(({key: 'ds:1'.*?)\\);</script>\", str(all_script_tags)))\n",
    "    \n",
    "    matched_images_data_fix = json.dumps(matched_images)\n",
    "    matched_images_data_json = json.loads(matched_images_data_fix)\n",
    "\n",
    "    # search for only suggested search thumbnails related\n",
    "    # https://regex101.com/r/ITluak/2\n",
    "    suggested_search_thumbnails = \",\".join(re.findall(r'{key(.*?)\\[null,\\\"Size\\\"', matched_images_data_json))\n",
    "\n",
    "    # https://regex101.com/r/MyNLUk/1\n",
    "    suggested_search_thumbnail_encoded = re.findall(r'\\\"(https:\\/\\/encrypted.*?)\\\"', suggested_search_thumbnails)\n",
    "\n",
    "    # zip() is used on purpose over zip_longest() as number of results would be identical\n",
    "    for suggested_search, suggested_search_fixed_thumbnail in zip(soup.select(\".PKhmud.sc-it.tzVsfd\"), suggested_search_thumbnail_encoded):\n",
    "        suggested_searches.append({\n",
    "            \"name\": suggested_search.select_one(\".VlHyHc\").text,\n",
    "            \"link\": f\"https://www.google.com{suggested_search.a['href']}\",\n",
    "            # https://regex101.com/r/y51ZoC/1\n",
    "            \"chips\": \"\".join(re.findall(r\"&chips=(.*?)&\", suggested_search.a[\"href\"])),\n",
    "            # https://stackoverflow.com/a/4004439/15164646 comment by Frédéric Hamidi\n",
    "            \"thumbnail\": bytes(suggested_search_fixed_thumbnail, \"ascii\").decode(\"unicode-escape\")\n",
    "        })\n",
    "\n",
    "    return suggested_searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting original resolution images\n",
    "\n",
    "Almost identical to extracting suggested search results except for different regular expressions:\n",
    "1. Create a temporary `list` `google_images` where extracted data will be appended.\n",
    "2. Extracting `all_script_tags`.\n",
    "3. Extracting `matched_images_data` to extract thumbnails and original resolution images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_images():\n",
    "\n",
    "    \"\"\"\n",
    "    https://kodlogs.com/34776/json-decoder-jsondecodeerror-expecting-property-name-enclosed-in-double-quotes\n",
    "    if you try to json.loads() without json.dumps() it will throw an error:\n",
    "    \"Expecting property name enclosed in double quotes\"\n",
    "    \"\"\"\n",
    "\n",
    "    google_images = []\n",
    "\n",
    "    all_script_tags = soup.select(\"script\")\n",
    "\n",
    "    # # https://regex101.com/r/48UZhY/4\n",
    "    matched_images_data = \"\".join(re.findall(r\"AF_initDataCallback\\(([^<]+)\\);\", str(all_script_tags)))\n",
    "    \n",
    "    matched_images_data_fix = json.dumps(matched_images_data)\n",
    "    matched_images_data_json = json.loads(matched_images_data_fix)\n",
    "\n",
    "    # https://regex101.com/r/VPz7f2/1\n",
    "    matched_google_image_data = re.findall(r'\\\"b-GRID_STATE0\\\"(.*)sideChannel:\\s?{}}', matched_images_data_json)\n",
    "\n",
    "    # https://regex101.com/r/NnRg27/1\n",
    "    matched_google_images_thumbnails = \", \".join(\n",
    "        re.findall(r'\\[\\\"(https\\:\\/\\/encrypted-tbn0\\.gstatic\\.com\\/images\\?.*?)\\\",\\d+,\\d+\\]',\n",
    "                   str(matched_google_image_data))).split(\", \")\n",
    "\n",
    "    thumbnails = [\n",
    "        bytes(bytes(thumbnail, \"ascii\").decode(\"unicode-escape\"), \"ascii\").decode(\"unicode-escape\") for thumbnail in matched_google_images_thumbnails\n",
    "    ]\n",
    "\n",
    "    # removing previously matched thumbnails for easier full resolution image matches.\n",
    "    removed_matched_google_images_thumbnails = re.sub(\n",
    "        r'\\[\\\"(https\\:\\/\\/encrypted-tbn0\\.gstatic\\.com\\/images\\?.*?)\\\",\\d+,\\d+\\]', \"\", str(matched_google_image_data))\n",
    "\n",
    "    # https://regex101.com/r/fXjfb1/4\n",
    "    # https://stackoverflow.com/a/19821774/15164646\n",
    "    matched_google_full_resolution_images = re.findall(r\"(?:'|,),\\[\\\"(https:|http.*?)\\\",\\d+,\\d+\\]\", removed_matched_google_images_thumbnails)\n",
    "\n",
    "    full_res_images = [\n",
    "        bytes(bytes(img, \"ascii\").decode(\"unicode-escape\"), \"ascii\").decode(\"unicode-escape\") for img in matched_google_full_resolution_images\n",
    "    ]\n",
    "    \n",
    "    for index, (metadata, thumbnail, original) in enumerate(zip(soup.select(\".isv-r.PNCib.MSM1fd.BUooTd\"), thumbnails, full_res_images), start=1):\n",
    "        google_images.append({\n",
    "            \"title\": metadata.select_one(\".VFACy.kGQAp.sMi44c.lNHeqe.WGvvNb\")[\"title\"],\n",
    "            \"link\": metadata.select_one(\".VFACy.kGQAp.sMi44c.lNHeqe.WGvvNb\")[\"href\"],\n",
    "            \"source\": metadata.select_one(\".fxgdke\").text,\n",
    "            \"thumbnail\": thumbnail,\n",
    "            \"original\": original\n",
    "        })\n",
    "\n",
    "        # Download original images\n",
    "        print(f\"Downloading {index} image...\")\n",
    "        \n",
    "        opener=urllib.request.build_opener()\n",
    "        opener.addheaders=[(\"User-Agent\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36\")]\n",
    "        urllib.request.install_opener(opener)\n",
    "\n",
    "        urllib.request.urlretrieve(original, f\"Bs4_Images/original_size_img_{index}.jpg\")\n",
    "\n",
    "        # Insert 4\n",
    "\n",
    "        # Insert 5\n",
    "\n",
    "            # Insert save within the for loop\n",
    "\n",
    "    return google_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Decode extracted encoded `thumbnails`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thumbnails = [\n",
    "    bytes(bytes(thumbnail, \"ascii\").decode(\"unicode-escape\"), \"ascii\").decode(\"unicode-escape\") for thumbnail in matched_google_images_thumbnails\n",
    "]\n",
    "\n",
    "# equvalent to \n",
    "for fixed_google_image_thumbnail in matched_google_images_thumbnails:\n",
    "    # https://stackoverflow.com/a/4004439/15164646 comment by Frédéric Hamidi\n",
    "    google_image_thumbnail_not_fixed = bytes(fixed_google_image_thumbnail, 'ascii').decode('unicode-escape')\n",
    "    # after first decoding, Unicode characters are still present. After the second iteration, they were decoded.\n",
    "    google_image_thumbnail = bytes(google_image_thumbnail_not_fixed, 'ascii').decode('unicode-escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Decode extracted encoded `full_res_images`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_res_images = [\n",
    "      bytes(bytes(img, \"ascii\").decode(\"unicode-escape\"), \"ascii\").decode(\"unicode-escape\") for img in matched_google_full_resolution_images\n",
    "  ]\n",
    "\n",
    "# equvalent to\n",
    "for index, fixed_full_res_image in enumerate(matched_google_full_resolution_images):\n",
    "    # https://stackoverflow.com/a/4004439/15164646 comment by Frédéric Hamidi\n",
    "    original_size_img_not_fixed = bytes(fixed_full_res_image, 'ascii').decode('unicode-escape')\n",
    "    original_size_img = bytes(original_size_img_not_fixed, 'ascii').decode('unicode-escape')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save full resolution images locally:\n",
    "\n",
    "- `urllib.request.build_opener()`:\tmanages the chaining of handlers and will automatically add headers on each request (row below).\n",
    "- `opener.addheaders[()]`:\tto add headers to the request.\n",
    "- `urllib.install_opener()`:\tset opener as a default global opener. Whatever that means 👀\n",
    "- `urllib.request.urlretrieve()`:\tto save images locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opener=urllib.request.build_opener()\n",
    "opener.addheaders=[(\"User-Agent\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36\")]\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "urllib.request.urlretrieve(original, f\"Bs4_Images/original_size_img_{index}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Google Images API\n",
    "- No need to figure out regular expressions, create a parser and maintain it over time, or how to scale the number of requests without being blocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serpapi_get_google_images():\n",
    "    image_results = []\n",
    "    \n",
    "    for query in [\"Coffee\", \"boat\", \"skyrim\", \"minecraft\"]:\n",
    "        # search query parameters\n",
    "        params = {\n",
    "            \"engine\": \"google\",               # search engine. Google, Bing, Yahoo, Naver, Baidu...\n",
    "            \"q\": query,                       # search query\n",
    "            \"tbm\": \"isch\",                    # image results\n",
    "            \"num\": \"100\",                     # number of images per page\n",
    "            \"ijn\": 0,                         # page number: 0 -> first page, 1 -> second...\n",
    "            \"api_key\": os.getenv(\"API_KEY\")   # your serpapi api key\n",
    "            # other query parameters: hl (lang), gl (country), etc  \n",
    "        }\n",
    "    \n",
    "        search = GoogleSearch(params)         # where data extraction happens\n",
    "    \n",
    "        images_is_present = True\n",
    "        while images_is_present:\n",
    "            results = search.get_dict()       # JSON -> Python dictionary\n",
    "    \n",
    "            # checks for \"Google hasn't returned any results for this query.\"\n",
    "            if \"error\" not in results:\n",
    "                for image in results[\"images_results\"]:\n",
    "                    if image[\"original\"] not in image_results:\n",
    "                        image_results.append(image[\"original\"])\n",
    "                \n",
    "                # update to the next page\n",
    "                params[\"ijn\"] += 1\n",
    "            else:\n",
    "                print(results[\"error\"])\n",
    "                images_is_present = False\n",
    "    \n",
    "    # -----------------------\n",
    "    # Downloading images\n",
    "\n",
    "    for index, image in enumerate(results[\"images_results\"], start=1):\n",
    "        print(f\"Downloading {index} image...\")\n",
    "        \n",
    "        opener=urllib.request.build_opener()\n",
    "        opener.addheaders=[(\"User-Agent\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36\")]\n",
    "        urllib.request.install_opener(opener)\n",
    "\n",
    "        urllib.request.urlretrieve(image[\"original\"], f\"SerpApi_Images/original_size_img_{index}.jpg\")\n",
    "\n",
    "    print(json.dumps(image_results, indent=2))\n",
    "    print(len(image_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f502ec625bf6bc95d53fa7e0a4248de33de8dc2f4d14405c1265ee3ea7a823af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
